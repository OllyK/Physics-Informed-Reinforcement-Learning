wandb_version: 1

_disable_action_flattening:
  desc: null
  value: false
_disable_execution_plan_api:
  desc: null
  value: true
_disable_preprocessor_api:
  desc: null
  value: false
_fake_gpus:
  desc: null
  value: false
_tf_policy_handles_more_than_one_loss:
  desc: null
  value: false
_wandb:
  desc: null
  value:
    cli_version: 0.13.4
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.9.14
    start_time: 1665660872.868684
    t:
      1:
      - 1
      - 30
      - 55
      2:
      - 1
      - 30
      - 55
      3:
      - 16
      - 23
      4: 3.9.14
      5: 0.13.4
      8:
      - 5
action_space:
  desc: null
  value: null
actions_in_input_normalized:
  desc: null
  value: false
adam_epsilon:
  desc: null
  value: 1.0e-08
algo_class:
  desc: null
  value: ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQN
always_attach_evaluation_results:
  desc: null
  value: false
batch_mode:
  desc: null
  value: truncate_episodes
before_learn_on_batch:
  desc: null
  value: null
buffer_size:
  desc: null
  value: -1
callbacks_class:
  desc: null
  value: ray.rllib.algorithms.callbacks.DefaultCallbacks
clip_actions:
  desc: null
  value: false
clip_rewards:
  desc: null
  value: null
collect_metrics_timeout:
  desc: null
  value: -1
compress_observations:
  desc: null
  value: false
count_steps_by:
  desc: null
  value: env_steps
create_env_on_local_worker:
  desc: null
  value: false
custom_evaluation_function:
  desc: null
  value: null
custom_resources_per_worker:
  desc: null
  value: {}
disable_env_checking:
  desc: null
  value: false
double_q:
  desc: null
  value: true
dueling:
  desc: null
  value: false
eager_max_retraces:
  desc: null
  value: 20
eager_tracing:
  desc: null
  value: false
enable_connectors:
  desc: null
  value: false
enable_tf1_exec_eagerly:
  desc: null
  value: false
env:
  desc: null
  value: deflector-v0
env_config:
  desc: null
  value:
    desired_angle: 70
    n_cells: 256
    wavelength: 1100
env_task_fn:
  desc: null
  value: null
evaluation_config:
  desc: null
  value:
    explore: false
evaluation_duration:
  desc: null
  value: 10
evaluation_duration_unit:
  desc: null
  value: episodes
evaluation_interval:
  desc: null
  value: null
evaluation_num_episodes:
  desc: null
  value: -1
evaluation_num_workers:
  desc: null
  value: 0
evaluation_parallel_to_training:
  desc: null
  value: false
evaluation_sample_timeout_s:
  desc: null
  value: 180.0
exploration_config:
  desc: null
  value:
    type: PerWorkerEpsilonGreedy
explore:
  desc: null
  value: true
extra_python_environs_for_driver:
  desc: null
  value: {}
extra_python_environs_for_worker:
  desc: null
  value: {}
fake_sampler:
  desc: null
  value: false
framework_str:
  desc: null
  value: torch
gamma:
  desc: null
  value: 0.99
grad_clip:
  desc: null
  value: 40
hiddens:
  desc: null
  value:
  - 256
horizon:
  desc: null
  value: null
ignore_worker_failures:
  desc: null
  value: false
in_evaluation:
  desc: null
  value: false
input_:
  desc: null
  value: sampler
input_config:
  desc: null
  value: {}
input_evaluation:
  desc: null
  value: -1
keep_per_episode_custom_metrics:
  desc: null
  value: false
learning_starts:
  desc: null
  value: -1
local_tf_session_args:
  desc: null
  value:
    inter_op_parallelism_threads: 8
    intra_op_parallelism_threads: 8
log_level:
  desc: null
  value: WARN
log_sys_usage:
  desc: null
  value: true
logger_config:
  desc: null
  value: null
logger_creator:
  desc: null
  value: null
lr:
  desc: null
  value: 0.0005
lr_schedule:
  desc: null
  value: null
max_requests_in_flight_per_replay_worker:
  desc: null
  value: .inf
max_requests_in_flight_per_sampler_worker:
  desc: null
  value: 2
metrics_episode_collection_timeout_s:
  desc: null
  value: 60.0
metrics_num_episodes_for_smoothing:
  desc: null
  value: 100
metrics_smoothing_episodes:
  desc: null
  value: -1
min_iter_time_s:
  desc: null
  value: -1
min_sample_timesteps_per_iteration:
  desc: null
  value: 25000
min_sample_timesteps_per_reporting:
  desc: null
  value: -1
min_time_s_per_iteration:
  desc: null
  value: 30
min_time_s_per_reporting:
  desc: null
  value: -1
min_train_timesteps_per_iteration:
  desc: null
  value: 0
min_train_timesteps_per_reporting:
  desc: null
  value: -1
model:
  desc: null
  value:
    custom_model: model
monitor:
  desc: null
  value: -1
n_step:
  desc: null
  value: 3
no_done_at_end:
  desc: null
  value: false
noisy:
  desc: null
  value: false
normalize_actions:
  desc: null
  value: false
num_atoms:
  desc: null
  value: 1
num_consecutive_worker_failures_tolerance:
  desc: null
  value: 100
num_cpus_for_local_worker:
  desc: null
  value: 1
num_cpus_per_worker:
  desc: null
  value: 1
num_envs_per_worker:
  desc: null
  value: 8
num_gpus:
  desc: null
  value: 1
num_gpus_per_worker:
  desc: null
  value: 0
num_workers:
  desc: null
  value: 32
observation_filter:
  desc: null
  value: NoFilter
observation_fn:
  desc: null
  value: null
observation_space:
  desc: null
  value: null
off_policy_estimation_methods:
  desc: null
  value: {}
optimizer:
  desc: null
  value:
    debug: false
    max_weight_sync_delay: 400
    num_replay_buffer_shards: 4
output:
  desc: null
  value: null
output_compress_columns:
  desc: null
  value:
  - obs
  - new_obs
output_config:
  desc: null
  value: {}
output_max_file_size:
  desc: null
  value: 67108864
placement_strategy:
  desc: null
  value: PACK
policies:
  desc: null
  value: {}
policies_to_train:
  desc: null
  value: null
policy_map_cache:
  desc: null
  value: null
policy_map_capacity:
  desc: null
  value: 100
policy_mapping_fn:
  desc: null
  value: null
postprocess_inputs:
  desc: null
  value: false
preprocessor_pref:
  desc: null
  value: deepmind
prioritized_replay:
  desc: null
  value: -1
prioritized_replay_alpha:
  desc: null
  value: -1
prioritized_replay_beta:
  desc: null
  value: -1
prioritized_replay_eps:
  desc: null
  value: -1
recreate_failed_workers:
  desc: null
  value: false
remote_env_batch_wait_ms:
  desc: null
  value: 0
remote_worker_envs:
  desc: null
  value: false
render_env:
  desc: null
  value: false
replay_batch_size:
  desc: null
  value: -1
replay_buffer_config:
  desc: null
  value:
    capacity: 2000000
    learning_starts: 50000
    no_local_replay_buffer: true
    prioritized_replay: -1
    prioritized_replay_alpha: 0.6
    prioritized_replay_beta: 0.4
    prioritized_replay_eps: 1.0e-06
    replay_buffer_shards_colocated_with_driver: true
    type: MultiAgentPrioritizedReplayBuffer
    worker_side_prioritization: true
replay_mode:
  desc: null
  value: independent
replay_sequence_length:
  desc: null
  value: null
restart_failed_sub_environments:
  desc: null
  value: false
rollout_fragment_length:
  desc: null
  value: 50
sample_async:
  desc: null
  value: false
sample_collector:
  desc: null
  value: ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector
sampler_perf_stats_ema_coef:
  desc: null
  value: null
seed:
  desc: null
  value: null
shuffle_buffer_size:
  desc: null
  value: 0
sigma0:
  desc: null
  value: 0.5
simple_optimizer:
  desc: null
  value: -1
soft_horizon:
  desc: null
  value: false
store_buffer_in_checkpoints:
  desc: null
  value: false
sync_filters_on_rollout_workers_timeout_s:
  desc: null
  value: 60.0
synchronize_filters:
  desc: null
  value: true
target_network_update_freq:
  desc: null
  value: 500000
tf_session_args:
  desc: null
  value:
    allow_soft_placement: true
    device_count:
      CPU: 1
    gpu_options:
      allow_growth: true
    inter_op_parallelism_threads: 2
    intra_op_parallelism_threads: 2
    log_device_placement: false
timeout_s_replay_manager:
  desc: null
  value: 0.0
timeout_s_sampler_manager:
  desc: null
  value: 0.0
timesteps_per_iteration:
  desc: null
  value: -1
train_batch_size:
  desc: null
  value: 512
training_intensity:
  desc: null
  value: 1
v_max:
  desc: null
  value: 10.0
v_min:
  desc: null
  value: -10.0
validate_workers_after_construction:
  desc: null
  value: true
